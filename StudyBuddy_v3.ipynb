{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHsGAJDfpnJwEVM8jCu4VD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppradyoth/ML-101-Workshop/blob/main/StudyBuddy_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x60TAFyCc7XQ",
        "outputId": "27b91717-d1de-410e-8cef-c850d2f36ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit pyngrok google-generativeai sentence-transformers faiss-cpu PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile studybuddy_app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import tempfile\n",
        "import concurrent.futures\n",
        "import textwrap\n",
        "\n",
        "# -------------------------------\n",
        "# Attempt to access the GOOGLE_API_KEY\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google API Key successfully loaded and configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing Google API Key: {e}\")\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is correctly set in Colab secrets.\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
        "\n",
        "document_text = \"\"\n",
        "chunks = []\n",
        "chunk_embeddings = None\n",
        "index = None\n",
        "chat_history = []\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=300, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def get_relevant_chunks(query, k=3):\n",
        "    query_vec = embedder.encode([query])\n",
        "    distances, indices = index.search(np.array(query_vec), k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def ask_studybuddy(query):\n",
        "    if not chunks:\n",
        "        return \"âš ï¸ Please upload and process a PDF first.\"\n",
        "\n",
        "    relevant_chunks = get_relevant_chunks(query)\n",
        "    context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "    prompt = f\"\"\"You are StudyBuddy, a helpful academic assistant.\n",
        "\n",
        "Use the context below to answer the user's question.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            answer = response.text.strip()\n",
        "            chat_history.append((query, answer))\n",
        "            return format_chat_history()\n",
        "    except concurrent.futures.TimeoutError:\n",
        "        return \"âŒ Gemini timed out.\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\"\n",
        "\n",
        "def generate_quiz():\n",
        "    if not document_text:\n",
        "        return \"âš ï¸ Please upload a PDF first.\"\n",
        "\n",
        "    prompt = f\"\"\"Generate 5 quiz questions from the following academic material.\n",
        "Provide a mix of MCQs and short answers.\n",
        "\n",
        "Material:\n",
        "\\\"\\\"\\\"\n",
        "{document_text[:1000]}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Format:\n",
        "Q1. [Question]\n",
        "A. [Option 1]\n",
        "B. [Option 2]\n",
        "C. [Option 3]\n",
        "D. [Option 4]\n",
        "Answer: [Correct Option or Short Answer]\"\"\"\n",
        "\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(model.generate_content, prompt)\n",
        "            response = future.result(timeout=20)\n",
        "            return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\"\n",
        "\n",
        "def format_chat_history():\n",
        "    return \"\\n\\n\".join([f\"ğŸ§‘â€ğŸ“ {q}\\nğŸ¤– {textwrap.fill(a, 100)}\" for q, a in chat_history])\n",
        "\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"StudyBuddy\", page_icon=\"ğŸ“˜\")\n",
        "st.title(\"ğŸ“˜ StudyBuddy: Ask Your PDF Anything\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"ğŸ“‚ Upload a PDF\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        pdf_path = tmp.name\n",
        "\n",
        "    with st.spinner(\"Processing PDF...\"):\n",
        "        document_text = extract_text_from_pdf(pdf_path)\n",
        "        chunks = chunk_text(document_text)\n",
        "        chunk_embeddings = embedder.encode(chunks)\n",
        "        index = faiss.IndexFlatL2(chunk_embeddings[0].shape[0])\n",
        "        index.add(np.array(chunk_embeddings))\n",
        "        chat_history.clear()\n",
        "\n",
        "    st.success(\"âœ… PDF processed. You can now ask questions or generate quizzes.\")\n",
        "\n",
        "st.header(\"ğŸ’¬ Ask a Question\")\n",
        "query = st.text_input(\"Enter your question about the PDF\")\n",
        "if st.button(\"Ask\"):\n",
        "    if query.strip():\n",
        "        with st.spinner(\"Generating answer...\"):\n",
        "            answer = ask_studybuddy(query)\n",
        "            st.text_area(\"ğŸ“š Answer\", value=answer, height=250)\n",
        "    else:\n",
        "        st.warning(\"Please enter a valid question.\")\n",
        "\n",
        "st.header(\"ğŸ“ Generate Quiz\")\n",
        "if st.button(\"Generate Quiz from PDF\"):\n",
        "    with st.spinner(\"Generating quiz...\"):\n",
        "        quiz = generate_quiz()\n",
        "        st.text_area(\"ğŸ§ª Quiz\", value=quiz, height=300)\n",
        "\n",
        "if chat_history:\n",
        "    st.header(\"ğŸ“– Chat History\")\n",
        "    st.markdown(format_chat_history())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLq8YYbdEuG",
        "outputId": "9b21eabd-e48a-4b8b-8c90-9fd712efd490"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing studybuddy_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n"
      ],
      "metadata": {
        "id": "nKq8alNRdMyL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Start Streamlit\n",
        "def run():\n",
        "    subprocess.Popen([\"streamlit\", \"run\", \"studybuddy_app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "threading.Thread(target=run).start()\n",
        "time.sleep(10)\n",
        "\n",
        "# Launch Cloudflared tunnel\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl6adaixd1s8",
        "outputId": "bde036e1-da54-4b7a-a2c9-d42f992aabe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-07-12T07:18:26Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-07-12T07:18:26Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m |  https://seventh-air-bra-laboratories.trycloudflare.com                                    |\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.7.0 (Checksum 51e3909335fd7ba2ed5c696b0a6fb7d4a74f6a15bf36615cea0fccba620cfb3f)\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.4, GoArch: amd64\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 6939d6df-b9a9-4fd6-9fc2-b4ed8dfabd80\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use :: as source for IPv6\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77\n",
            "2025/07/12 07:18:29 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-07-12T07:18:29Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mad221996-c9f0-4947-b0ac-e7b6f8731ff7 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.77 \u001b[36mlocation=\u001b[0mord14 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    }
  ]
}